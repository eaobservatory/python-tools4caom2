#!/usr/bin/env python

__author__ = "Russell O. Redman"

import commands
import logging
import os
import os.path
import re
import time

from tools4caom2 import __version__
from tools4caom2.basecontainer import basecontainer
from tools4caom2.data_web_client import data_web_client

__doc__ = """
The dataproc_container class holds a list of processed files to ingest that are
referenced by their identity_instance_id in the data_proc
dp_recipe_output table

Version: """ + __version__.version

class dataproc_container(basecontainer):
    def __init__(self, 
                 log,
                 data_web_client,
                 identity_instance_id, 
                 conn, 
                 working_directory, 
                 filterfunc):
        """
        A dataproc_container references the list of AD URIs generated by
        a recipe instance in the data_proc system running at the CADC.  To 
        use a dataproc_container, it is necessary to be able to connect to
        the data_proc database at the CADC.
        
        The recipe instance is identified  using the primary key
        identity_instance_id for the data_proc.dbo.dp_recipe_output table.
        This table contains a list of AD URIs with the format:
            "ad:[A-Z]+/(a-zA-Z.-_]+)"
        For example:
          ad:JCMT/jcmth20110811_00044_01_reduced001_nit_000
          ad:JCMT/jcmth20110811_00044_01_rsp_nit_000
        URIs found in the table will be checked to verify that the 
        file exists in AD before the file_id is added to the filedict.

        The list is valid only if the state of the recipe instance from the
        field dp_recipe_instance.dbo.state (also keyed using
        identity_instance_id) has the value "Y", indicating successful
        completion of the last run of the recipe instance.  An exception
        will be raised if any other state is encountered.
        
        The files will be extracted from AD into working_directory and after
        use will be deleted again.
        
        Arguments:
        log: a tools4caom2.logger object
        data_web_client: a tools4caom2.data_web_client object
        identity_instance_id: a string providing the primary key for the 
                              db_recipe_output table
        conn: a connection to the database
        working_directory: path to the working directory where temporary 
                           files can be created
        filterfunc: returns True if filename should be ingested
        """
        basecontainer.__init__(self, 'dp_' + identity_instance_id, log)

        if os.path.isdir(working_directory):
            self.directory = os.path.abspath(working_directory)
        else:
            self.log.console('not a directory: ' +  working_directory,
                             logging.ERROR)

        self.dataweb = data_web_client
        
        if not conn:
            self.log.console('no connection to the database',
                             logging.ERROR)

        self.archive = {}

        sqlcmd = '\n'.join([
            'SELECT state',
            'FROM data_proc.dbo.dp_recipe_instance',
            'WHERE identity_instance_id=' + identity_instance_id])
        result = conn.read(sqlcmd)
        
        if result:
            dp_state = result[0][0]
            if dp_state != 'Y':
                self.log.console(
                    'state of indentity_instance_id=' +
                    str(identity_instance_id) + ' is "' + dp_state + 
                    '" but must be "Y: for ingestion to proceed',
                    logging.ERROR) 

        sqlcmd = '\n'.join([
            'SELECT dp_output',
            'FROM data_proc.dbo.dp_recipe_output',
            'WHERE identity_instance_id=' + identity_instance_id])
        result = conn.read(sqlcmd)
        
        if result:
            filecount = 0
            for aduri, in result:
                match = re.search(r'ad:([A-Z]+)/([a-zA-Z0-9.\-_]+)',
                                 aduri)
                if match:
                    (archive, file_id) = match.group(1, 2)
                    # Beware that uri's from dp_recipe_output include extensions
                    file_id = os.path.splitext(file_id)[0].lower()
                    headers = self.dataweb.info(archive, file_id)
                    if headers and 'content-disposition' in headers:
                        m = re.match(r'^.*?filename=(.+)$', 
                                     headers['content-disposition'])
                        if m:
                            adfilename = m.group(1)
                            # use the adfilename to filter, but do not record 
                            # the name on disk, which will change when the 
                            # cutout for the primary header is done in get
                            if not filterfunc or filterfunc(adfilename):
                                self.filedict[file_id] = ''
                                self.archive[file_id] = archive
                                filecount += 1
                        else:
                            self.log.console('data web service cannot find ' +
                                             archive + '/' + file_id,
                                             logging.ERROR)
            if filecount == 0:
                self.log.console('identity_instance_id ' + 
                                 identity_instance_id + 
                                 ' contains no valid ad URIs',
                                 logging.ERROR)

    def get(self, file_id):
        """
        Fetch a file from ad into the working directory

        Arguments:
        file_id : The file_id to extract from the archive
        """
        if file_id not in self.archive:
            self.log.console('requesting bad file_id: ' + file_id +
                             ' from ' +repr(self.file_id_list()),
                             logging.ERROR)

        # This fetches only the header from the primary HDU, which
        # should result in significant performance improvements
        filepath = self.dataweb.get(self.archive[file_id], 
                                    file_id,
                                    params=data_web_client.PrimaryHEADER)
        if not filepath:
            self.log.console('could not get ' + file_id + ' from ' + 
                             self.archive[file_id],
                             logging.ERROR)
        self.filedict[file_id] = filepath
        return filepath

    def cleanup(self, file_id):
        """
        Clean up deleted the file from the working directory

        Arguments:
        file_id : file_id of the file to delete
        """
        os.remove(self.filedict[file_id])
